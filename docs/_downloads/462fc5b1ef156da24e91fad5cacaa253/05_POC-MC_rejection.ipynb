{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\nfrom pyvista import set_plot_theme\nset_plot_theme('document')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 05 - Monte Carlo rejection and ensemble conditioning\n \nThe created geological models with gempy were exported as SHEMAT-Suite input files. `SHEMAT-Suite <https://git.rwth-aachen.de/SHEMAT-Suite/SHEMAT-Suite-open>`_ [1] is a code for \nsolving coupled heat transport in porous media. It is written in fortran and uses a finite differences scheme in a hexahedral grid.\nIn this example, we will load a heat transport simulation from the base POC model we created in \"Geological model creation and gravity simulation\". We will demonstrate methods contained \nin OpenWF for loading the result file, displaying the parameters it contains and how to visualize these parameters. Finally, we will calculate the conductive heat flow and plot it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# import libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport sys\nsys.path.append('../../')\n\nimport OpenWF.postprocessing as pp\nimport random\nimport gempy as gp\nfrom gempy.bayesian.fields import probability, information_entropy\n\nimport matplotlib.pyplot as plt\n\nprint(f\"Run mit GemPy version {gp.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rejection algorithm based on random walk\nWe created a tiny ensemble of 10 different SHEMAT-Suite models in the previous step and will use a rejection algorithm to get a posterior ensemble of models.  \nFor this, we \"borrow\" the Metropolis acceptance probability which is defined as:  \n\n\\begin{align}\\alpha(x_{t-1},z) = \\begin{cases} min\\big(\\frac{p(z)}{p(x_{t-1})},1\\big), & \\text{if } p(x_{t-1}) > 0\\\\\n  1, & \\text{if } p(x_{t-1}) = 0 \\end{cases}\\end{align}\n\nA different approach would be to assess the missfit (as RMS error) of each realisation.  \n\n\\begin{align}\\alpha(x_{t-1},z) = \\begin{cases} exp\\big(-\\frac{S(z) - S(x_{t-1}) }{u_T}\\big), & \\text{if } S(z) > S(x_{t-1})\\\\\n  1, & \\text{otherwise }  \\end{cases}\\end{align}\n\nWe will use the second approach for now. As discretization error, we take a value from Elison(2015), $u_{T-discr} = 0.7$ K, an estimate of error. This error should \nbe estimated to best knowledge.  \n\nUsing Gauss error propagation, we assess a potential error for the realisations.  \n\n\\begin{align}\\end{align}\nu_T = \\sqrt{\\big(\\frac{\\partial T}{\\partial x_1}u_1 \\big)^2 + ... + \\big(\\frac{\\partial T}{\\partial x_n}u_n \\big)^2} \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Literature sources for log-errors:\n_The lower part of the disturbed log profile (below the cross-over point) was rotated to match these corrected temperatures. \nIn the upper part of the profile, the same correction as for method A was applied. The quality of this correction method strongly depends on the correct calculation of the lowermost profile \ntemperatures. According to F\u00f6rster (2001), most of the corrected tem-peratures have errors of \u00b1 3 to 5 K._ https://doi.org/10.1186/s40517-020-00181-w  \n\n _The effective accuracy of commercial temperature logs is \u00b10.5\u00baC (Blackwell and Spafford, 1987)._  http://www.sprensky.com/publishd/temper2.html  \n\n _More normal accuracies are +- 0.25 \u00b0C over 0-200 \u00b0C_ Keith Geothermal Energy lecture  \n\n For errors as a function of e.g. logging speed, measurement response time etc, look https://doi.org/10.1016/j.petrol.2020.107727\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model preparation\nTo see, where our data points are situated, we load the model topography and plot the position of gravity stations and temperature boreholes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# import DTM\ndtm = np.load('../../models/Graben_base_model_topography.npy')\n\n# load base model\nmodel_path = '../../models/2021-06-04_POC_base_model/'\n\ngeo_model = gp.load_model('POC_PCT_model', \n                          path=model_path, recompile=False)\n\n# get delx and dely of the model, so cell sizes\ndelx = geo_model._grid.regular_grid.dx\ndely = geo_model._grid.regular_grid.dy\ndelz = geo_model._grid.regular_grid.dz\n\n\n# import gravity data and borehole locations\ng_data = pd.read_csv('../../models/2021-06-16_grav_of_POC_base_model.csv')\nbhole = np.array([[31, 14],\n                 [78, 22],\n                 [53, 34],\n                 [49, 44]])\n\n# plot the map\nfig = plt.figure(figsize=[15,7])\ncs = plt.contourf(dtm[:,:,0], dtm[:,:,1], dtm[:,:,2],20, cmap='gist_earth')\nplt.contour(dtm[:,:,0], dtm[:,:,1], dtm[:,:,2],10, colors='gray', zorder=1)\nplt.scatter(g_data['X'], g_data['Y'], marker='s', s=150, c='brown', edgecolor='k', \n            label='gravity stations', zorder=2)\nplt.scatter(bhole[:,0]*delx, bhole[:,1]*dely, marker='^', s=200, c='k', label='boreholes',\n           zorder=3)\nplt.colorbar(cs, label='elevation [m]')\nplt.legend(frameon=True)\nplt.xlabel('X [m]')\nplt.ylabel('Y [m]');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Lithology Blocks\nFirst let's load the lithology block of all 10 models, looking at the probabilities of the graben unit and at the model entropy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load and calculate Probability and Entropy using GemPy bayesian field functions\nens = np.load('../../data/outputs/MCexample_10realizations.npy')\nprior_prob = probability(ens)\nprior_entr = information_entropy(prior_prob)\n\nlayer = 5\n# upper filling\ngp.plot_2d(geo_model,\n                  show_lith=False, show_boundaries=False, show_data=False,\n                  regular_grid=prior_prob[layer],\n                  kwargs_regular_grid={'cmap': 'viridis',\n                                        'norm': None})\n# lower filling\ngp.plot_2d(geo_model,\n                  show_lith=False, show_boundaries=False, show_data=False,\n                  regular_grid=prior_prob[layer-1],\n                  kwargs_regular_grid={'cmap': 'viridis',\n                                        'norm': None});\n\np2dp = gp.plot_2d(geo_model,\n                  show_lith=False, show_boundaries=False, show_data=False,\n                  regular_grid=prior_entr,\n                  kwargs_regular_grid={'cmap': 'magma',\n                                       'norm': None,\n                                      'colorbar': True}\n                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Information entropy plot shows where the maximal Uncertainty is in our model, i.e. where the contacts are between the graben units and the basement. \nA lot of uncertainty is visible in the right part of the model (between around 16000 and 20000), where the main graben unit may or may not be present.\n\n## Gravity rejection\nIn a first stage, we take a look at the gravity signal of each realization. The gravity signal is \"recorded\" at each of the squares you see in the plot above. \nComparing the recorded gravity signals of each realization with the ones of the base model (which we regard as the \"true\" observations), we can differentiate between fitting and non-fitting ensemble \nmembers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g_simu = pd.read_csv('../../data/outputs/MCexample_10grav.csv')\ng_simu.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember the $u_T$ from previously? Here, we estimate it from an artificially superimposed noise on the data. As our \"observed data\" is actually just the simulated gravity from the base model, \nit does not have noise. That's why we artificially add it. This would not be needed with real data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "add_noise = True\nif add_noise==True:\n    np.random.seed(27)\n    noise = np.random.normal(0, 1., size=15)\n    g_data_noise = g_data.copy()\n    g_data_noise['grav'] = g_data_noise['grav'] + noise\n    print(np.mean(noise))\n    u_g = np.mean(noise)\nelif add_noise==False:\n    u_g = 0.5\n\n#calculate stdeviation and mean of the prior ensemble\ng_simu_stdev = g_simu.std(axis=1)\ng_simu_mean = g_simu.mean(axis=1)\n\nfig = plt.figure(figsize=[15,7])\ncs = plt.contourf(dtm[:,:,0], dtm[:,:,1], dtm[:,:,2],20, cmap='gist_earth')\nplt.contour(dtm[:,:,0], dtm[:,:,1], dtm[:,:,2],10, colors='gray', zorder=1)\ncs = plt.scatter(g_data['X'], g_data['Y'], c=g_simu_stdev, marker='s', \n                 s=100, zorder=2, cmap='magma')\nplt.xlabel('x (m)')\nplt.ylabel('y (m)')\nplt.colorbar(cs, label='standard deviation');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the plot above, we see the distribution of the standard deviation of our gravity stations, so already where the most \"sensitive\" stations are. For a better performing rejection, it may be suitable\nto remove redundant stations, i.e. once with a very low standard deviation.\nNow, for the MonteCarlo rejection step, we use an implemented method `rejection`, which goes through the RMSE vector of our realizations and compares the RMSE of each realization. The ones with relatively\nlower RMSE will get chosen:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rmse = pp.c_rmse(g_simu, g_data['grav'])\naccept_g, P = pp.rejection(rmse=rmse, rnseed=random.seed(7), u_g=u_g, median=False)\nprint(f\"Accepted realizations are {accept_g}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The RMSE of our realizations is:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having accepted some of the initial 10 realizations, we can again calculate the probability field for different units \nand the model entropy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accepted_reals = ens[accept_g, :]\ngrav_prob = probability(accepted_reals)\ngrav_entr = information_entropy(grav_prob)\n\n\np2dp = gp.plot_2d(geo_model,\n                  show_lith=False, show_boundaries=False, show_data=False,\n                  regular_grid=grav_entr,\n                  kwargs_regular_grid={'cmap': 'magma',\n                                       'norm': None}\n                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing this to the Entropy plot from above, we see that the overall entropy is reduced in many parts of the model. Also the \"thickness\" of the areas with increased entropy is reduced, hinting at a\nsucessful reduction of depth uncertainty for, e.g. the graben units. We now go ahead and save the lithology blocks of the accepted realizations, as these could now be used for subsequent\nheat tranpsort simulations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.save('../../data/outputs/lith_blocks_accepted.npy', accepted_reals)\nnp.savetxt('../../data/outputs/accepted_realizations_ID.txt', accept_g, fmt='%d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember how in a previous tutorial step (\"Create SHEMAT-Suite models\"), we created SHEMAT-Suite models for the whole 10 realizations, i.e. for the whole _apriori_ ensemble? \nFollowing the workflow with sequentially constraining the model space, we wouldn't actually need to create a SHEMAT-Suite model for every ensemble member, but just for the accepted realizations. \nWhich means, in this case:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Realizations accepted: {accept_g}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means, we'd only need to run heat-transport simulations for the realizations accepted by the gravity rejection step.\n\n## Temperature rejection\nThe black triangles in the Map plot are the locations from 4 different boreholes in the model. Temperature data from these boreholes is now used in a similar fashion to further reduce the model to \nrealizations, which now fit both the gravity and the temperature signal.\n\nSimilarly to the previous tutorial, where we saved the base model as a SHEMAT-Input file, we now do the same with the accepted realizations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f = h5py.File('../../models/SHEMAT-Suite_output/SHEMAT_PCT_base_model_temp_final.h5','r')\n\nz,y,x = f['uindex'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The openWF package hase one plotting function for displaying arbitary slices through the SHEMAT model, as presented in a previous tutorial step. Here, we have a look at the temperature field\nof the base model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[15,7])\npp.plot_slice('../../models/SHEMAT-Suite_output/SHEMAT_PCT_base_model_temp_final.h5',\n              parameter='temp', cell_number=25, direction = 'y', model_depth=6500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the previous step, where we estimated $u_g$, we now have to estimate $u_T$ for temperature. There we use some literature estimations for errors introduced in measurements of\nborehole temperatures:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define uT\nT_error = 0.25 # temperature error tool accuracy\ns_error = pp.fahrenheit_to_celsius(1.25, difference=True) # sensor response time of 2 sec and 1 year after drilling\nl_error = pp.fahrenheit_to_celsius(1.25, difference=True) # logging speed of 20/ft after 1 year\nd_error = 1.0 # estimated temperature error by discretization\n\nu_T = np.sqrt(T_error**2 + s_error**2 + l_error**2 + d_error**2)\nprint(u_T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SHEMAT-Suite, in a specific operation mode, writes ASCII files, ending on `.dat`. In these files, SHEMAT-Suite provides information about simulated variables (in our case temperature) \ncompared to observed ones. OpenWF has a routine to read in these `.dat` files and display them as a pandas dataframe:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pp.load_inv('../../models/SHEMAT-Suite_output/POC_base_model_final.dat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's load all these simulation files from our ensemble. As we already simulated all 10 realizations of the apriori ensemble, we load all 10 dat-files. However, in a sequential \nconditioning workflow, we'd just have the simulations from realizations accepted by the gravity conditioning step.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "outp_path = '../../models/SHEMAT-Suite_output/'\n\ndiffs = np.loadtxt(outp_path+f'POC_MC_{accept_g[0]}_final.dat',skiprows=3,usecols=(8,),dtype=float)\nfor i in accept_g[1:]:\n    n = np.loadtxt(outp_path+f'POC_MC_{i}_final.dat',skiprows=3,usecols=(8,),dtype=float)\n    diffs=np.vstack([diffs,n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The diffs array we now created consists of the stacked 9th columns of each `.dat` file in the accepted realizations. Which means, as we have in total 5 accepted realizations, that array has 5 rows and \n128 columns (i.e. the number of measuring points).\n\nAs we already have the differences between observed and simulated values here (so difference between the columns `calc` and `obs` in the dataframe above), we do not need to use the \n`calc_rmse` method from above. Instead, we calculate it directly drom the diffs array, by first calculating the Sum of Squared Residuals (SSR) and then the RMSE:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# calculate the Sum of Squared Residuals\ndiffs_sq = diffs**2\nssr = diffs_sq.sum(axis=1)\n\n# calculate RMSE of each realisation.\nn = diffs.shape[1] # as we have 5 data sources / boreholes for temperature\nrmse_T = np.sqrt((ssr/n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now continue to work with the `rmse` array, but for having a complete information array, we stack it to the `diffs` array. This can be neat, e.g. for storing the diffs and RMSE in one file. \nWhen we stack the calculated parameters, we'll end up with an array with 130 columns. The first 128 columns are the differences between observed and calculated values, the 129th the SSR, and the \n130th column the RMSE.\n\nTo have information, which realizations (after constraining from gravity) these differences belong to, we finally add a 131st column, containing the realization number:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_diffs = np.column_stack((diffs,ssr,rmse_T))\n# add index to the realizations\nind = np.array(range(total_diffs.shape[0]))\ntotal_diffs = np.column_stack((total_diffs,accept_g))\nprint(total_diffs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optionally, we can then save it in an ASCII file:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.savetxt('../../models/SHEMAT-Suite_output/differences_RMSE.txt', total_diffs, fmt='%.4f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rejection sampling Temperature\nSimilar to the gravity rejection step before, we now work with the temperature RMSE to reject samples. However, this step might be somewhat ambigous: \nBecause we're only looking at conductive heat transport in this tutorial example, the sole difference between realizations will be based on the thickness of the geological unit with its thermal \nconductivtiy. As thermal conductivity does not vary as much as, let's say permeabiliy, the effect on the overall temperature field will likely be small.\n\nConsidering the estimated error above, the error might as well be in the same region as the differences of the simulations, yielding an unsatisfactory rejection:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accept_T, P_T = pp.rejection(rmse=rmse_T, rnseed=random.seed(1), u_g=u_T)\n\naccept_T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To show, what the simple implemented rejection method does, we write it out in the following code segment. We chronologically go through the rmse array, as this is the result of a simple MC simulation.\nThis means, this start from 1 to N can be used here, if samples generated are already in a random order and not correlated. That is usually the case with GemPy exports to SHEMAT-Suite.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First we fix the random seed of this Jupyter cell to the same as the previous method\nrandom.seed(1)\n# The RMSE is in the 130th column. With Python indexing starting at 0, this means we choose column 129\ncol = 129\n\n# We choose a reference. There are two options in the implemented method. \n# One, to start from the median value, as shown here, one for starting at the first realization.\nRef = np.median(total_diffs[:,col])\naccept = []\nP = []\nk=0\n# then we iterate through the different RMSE values\nfor i in range(total_diffs.shape[0]):\n    # if the current iteration has a smaller RMSE then the reference, we take it and mark it as the\n    # new reference\n    if total_diffs[i,col] < Ref:\n        Ref = total_diffs[i,col]\n        accept.append(i)\n    \n    # else we only accept it with a certain probability, defined by the exponential in the equation\n    # at the beginning\n    elif random.random() < np.exp(-(total_diffs[i,col] - Ref)/(u_T)):\n        P.append(np.exp(-(total_diffs[i,col] - Ref)/(u_T)))\n        Ref = total_diffs[i,col]\n        accept.append(i)\n        \nprint(f\"Accepted realizations are: {accept}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the model space now reduced to three models, we can calculate for a final time the probability of the model units and entropy of the model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accepted_reals_T = accepted_reals[accept, :]\ngrav_T_prob = probability(accepted_reals_T)\ngrav_T_entr = information_entropy(grav_T_prob)\n\np2dp = gp.plot_2d(geo_model,\n                  show_lith=False, show_boundaries=False, show_data=False,\n                  regular_grid=grav_T_entr,\n                  kwargs_regular_grid={'cmap': 'magma',\n                                       'norm': None}\n                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above shows a strong \"binary\" entropy field. Entropy is maximum (bright color) especially with respect to the depth of the post-graben unit interface. \nThe area of interest, however, the depth of the graben is now significantly reduced. We see, that the two resulting models do not differ that much with respect to graben depth, but only depth of the \npost-graben unit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"So, the final realizations which remain from the original {[accept_g[real] for real in accept]}!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}