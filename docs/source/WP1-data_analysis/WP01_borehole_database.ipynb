{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Temperature database usage\n\nIn the following, I present ways to interact with a database of borehole temperatures in the study area, using the module `db_access`.\nThese temperature data are used in other workpackages, e.g. for model calibration or MC-rejection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Borehole Database\n\nIt is no surprise that the core of a data-driven project is data. Data which is organized in a specific manner, so that users can easily access,\nanalyse, and manipulate data. Many different schemes exist, in which data can be organized, with a most common one being spreadsheet-like tables.\nThus, spreadsheet software like Microsoft Excel or Libre-Office Calc are among popular solutions, when it comes to working with data.\n\nWith growing amount of data, however, these software solutions may soon meet their limits, as they can get overly complicated. One example would be many :code:`.xls` files,\nwhich are connected among each other using hyperlinks. This is obviously an error-prone solution, not really practical. Thus, greater amounts of data with a more complex structure,\nare usually maintained in a `database <https://en.wikipedia.org/wiki/Database>`_, following a certain `data model <https://en.wikipedia.org/wiki/Data_model>`_.\nHere, we use `SQLITE <https://www.sqlite.org/index.html>`_ \\cite{hipp_sqlite_2019} as underlying database solution, a SQL database engine.\n\nIn the following, we will briefly describe the database structure, its content and provide some short examples how to access the database and work with the stored data.\n\n## Data model\n\nWithin the database, we follow, as we use SQL, a `relational model <https://en.wikipedia.org/wiki/Relational_model>`_ to organize stored data.\nThis data comprises mainly borehole temperature measurements in the study area. The data was originally compiled by Sch\u00e4rli and Kohl \\cite{scharli_archivierung_2002} in a set of excel tables. \nThis *original* data, i.e. in its excel form, is available as supplementary material to the NAGRA Working report\n`NAB 12-61 <https://www.nagra.ch/de/cat/publikationen/arbeitsberichte-nabs/nabs-2012/downloadcenter.htm>`_. \nThis report comprises temperature measurements for boreholes all over Switzerland. Additionally, a stratigraphical description is available for some boreholes. \nFigure \\ref{fig:borehole_map} shows boreholes in Switzerland, which are deeper than 500 m. \n\n\nMany of the temperature data from these deep boreholes is compiled in Sch\u00e4rli and Kohl \\cite{scharli_archivierung_2002}, in addition to temperature data from *shallow* boreholes, i.e. shallower than 500 m.\nIn this work, we use a subset of this data which is (**a**) inside our area of interest, and (**b**) publicly available data. \nFor instance, figure \\ref{fig:database_map} shows a subset of deep boreholes (triangles) in the study area, colored by the data restriction. \nWhile blue represents open data, boreholes colored in red contain confidential data. Within the database, this information is stored, so confidential data can easily be erased from the database, \nin case it is made public.\n\n\nCurrently, the database contains three related tables:\n* general borehole information (coordinates, name, original source, ...)  \n* temperature depth information for all boreholes  \n* available petrophysical information  \n\nNot much petrophysical data is available from the boreholes. Temperature depth information, however, comprises more than 39000 data points. \nIn the following, we present methods and procedures to access these data and work with it from within this notebook. For this, we use a mixture of SQL queries and methods of the data analysis library \n`pandas <https://pandas.pydata.org/>`_. \n\n## Acessing data and visualizing\nQuerying a database is maybe the most often performed task, when it comes to databases. When you type something in a seach bar, for example, you query a database for the words you are looking for. \nThe same, though in a more rudimentary form, can be done with the compiled \"borehole temperature\" database. \n\nThe following code cells in this notebook show how:\n* to connect to the database  \n* introduces a very small library :code:`db_access`\n* get information about available tables in the database\n* formulate queries to get desired data, e.g. temperature depth pairs for a specific borehole\n* store query results in a pandas dataframe and visualize them  \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nimport platform\nsys.path.append('../..')\nimport OpenWF.db_access as db_access\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "relative path to the .db file, which is the actual database\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "os_name = platform.system()\nif os_name=='Linux':\n    db_path = '../../../../ETHeatflow/dbase_model_btemps.db'\nelif os_name=='Windows':\n    db_path = '../../../../polybox/data_boreholes_aargau/interim/data_nagra_12-61/d_nab_database/dbase_model_btemps.db'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "connect to the database and get information about stored tables with the ``connect`` routine\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conn, c = db_access.connect(db_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we successfully connected to the database. One next step would be to see, what different tables are stored in the database. :code:`db_access` provides you with methods to do so. \nOf course, one can directly use an SQL query to do so. For user convenience, such queries are wrapped in some python methods of :code:`db_access`. \nFor instance, let's check the names of tables in the database:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\nprint(c.fetchall())\n\ndb_access.get_tables(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essentially, these two commands do the same thing. In the :code:`db_access` method, the :code:`c.execute` and :code:`c.fetchall` commands are bundled in one method, :code:`.get_tables()`. \nThe result are the three tables:  \n* borehole_information_temperatures  \n* temperature_data (with one backup table, marked with extension \\_bak)  \n* sample_information_petrophysics  \n\nIn its current state, `db_access` comprises very basic query methods. More specific data-queries still need to be done via the `c.execute` and `c.fetchall` chain which is extremely versatile.  \nFor instance, consider out of the over 30000 data entries, we want to get all temperature measurements for Borehole Nr. 111. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT * FROM {tn} WHERE {idf}=111;\".format(tn='temperature_data', idf='Nr'))\nprint(c.fetchall())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the name of this borehole, we can relate to the table *borehole_information_temperatures* and query the name for the borehole with Nr. 111 in the exact same way:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT {param} FROM {tn} WHERE {idf}=111;\".format(param='Namenach',\n                                                             tn='borehole_information_temperatures', idf='Nr'))\nprint(c.fetchall())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To know which columns are available to choose from as `{param}` in the `execute` command, we can either list names fetched by an `execute` command:\n\n\nnam = c.execute(\"select * from borehole_information_temperatures\")\nnames = list(map(lambda x: x[0], nam.description))\nprint(names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... or use a `db_access` method which returns this list of table headers:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "db_access.get_columns(c,table='borehole_information_temperatures')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now back to the query above, where we asked the database to provide all data for borehole *Riehen-1*, i.e. borehole Nr. 111. The query returns a list of table rows fitting the query command. \nWhile usable, it is difficult to read, at least for humans. This is, where pandas comes into play. As an extensive data analysis library, `pandas <https://pandas.pydata.org/>`_ provides a lot of tools \nto deal with a database and present them in `dataframes <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_, which can be displayed in a way more organized way. \nBelow, we submit a query for the temperature data for borehole Nr. 111 and display it.\nquery database for Borehole Nr. 111 and store it in the dataframe df.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.read_sql_query(\"select * from temperature_data where Nr = 111;\", conn)\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next to readability, another advantage of querying via pandas, and storing the result in a dataframe, is visualization. Pandas features some plotting functions, which can quickly plot parameters in a \ndataframe. For example, let's plot `Depth` versus `Temperature`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df.plot.scatter(x='Temperature', y='Depth_asl', s=50)\nplt.show()\n\n# alternative syntax in classical matplotlib\nfig, ax = plt.subplots()\nax.scatter(df['Temperature'], -df['Depth'], s=50)\nax.set_ylabel('Depth [m]')\nax.set_xlabel('Temperature [\u00b0C]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colleagues at `Georesources Switzerland Group <https://georessourcen.ethz.ch/en/#georesources-switzerland>`_ evaluated temperature data from deep boreholes following certain criteria. \nAccording to their analysis, a subset of the deep boreholes contain enough data for a reliable heat-flow estimation. Boreholes passing this quality assessment are marked with white **+** \nin Figure \\ref{fig:chosen_boreholes}. \n\n<hr>\n\\begin{figure}\n    \\includegraphics[width=10cm]{https://i.ibb.co/m5P5fCc/Base-Map-boreholes-database-valid-loic.png}\n    \\caption{\\label{fig:chosen_boreholes} Map of the study area, similar to Figure 2. Boreholes passing the quality assessment step are marked with white **+**.}\n\\end{figure}\n<hr>\n\nIt should be noted, that data in these boreholes are all temperature logs with a high enough data density to reliably assess a temperature gradient.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Until now, SQL queries consisted mainly of :code:`select * ...` where the * represents *all*, i.e. selecting everything (similar to an `ls *` listing every content of a folder in bash). \n# If now we want to know, for instance, all different Borehole numbers, which are the database ID for each borehole, we can use `select distinct ...`.\n\nall_borehole_numbers = pd.read_sql_query(\"select distinct Nr from temperature_data;\", conn)\nall_borehole_numbers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Out of this distinct list, only a few passed the QA step by the GS-Group. The ID-Numbers of these boreholes are compiled in the list below:\nHere is a list of boreholes which passed the QA step\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "borehole_numbers = [9, 10, 12, 21, 77, 78, 108, 111, 112, 113, 116, 122, 128, 139]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chaining queries\nWhen one searches for multiple keywords in, for instance, a google search, this narrows the number of potential results. Similarly this can be done when querying the database. \nAs an example, let's query all data from the boreholes, which pass the QA-workflow by the GS-Group, and plot temperatures versus depth:\nFor instance, we want to get all the boreholes marked as _valid_ in one dataframe:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "valids = pd.read_sql_query(\"select * from temperature_data where \"+\n                        \" OR \".join([\"Nr = {}\".format(n) for n in borehole_numbers]), conn)\nboreholes = pd.read_sql_query(\"select * from borehole_information_temperatures where \"+\n                              \" OR \".join([\" Nr = {}\".format(n) for n in borehole_numbers]), conn)\n\nfig = plt.figure(figsize=[10,8])\nplt.scatter(valids['Temperature'], valids['Depth_asl'], s=50, c=valids['Nr'], cmap='tab10', alpha=.5)\nplt.xlabel('Temperature [\u00b0C]')\nplt.ylabel('Depth [m asl]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this plot, where temperature measurements are colored by borehole number, we see that the temperature measurements from different boreholes overall follow a similar gradient. \nThere are, however, singular points next to the dense cluster of continuous temperature logs. These are data points from different measuring procedures, such as **B**ottom **H**ole **T**emperatures (BHTs).  \nIf one would like to include *only* temperature logs in a database query, this can easily be done by extending the above chained query command with an `AND` keyword, so that a query would read:  \n\n.. code-block:: SQL  \n\n   SELECT * FROM temperature_data WHERE Method = 'HRT' AND (Nr = ? OR Nr = ? OR ...);\n\n\nThis method essentially queries if a temperature measurement belongs to a borehole with the number specified in our `borehole_numbers` list, and if the measurement method is HRT.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "valids = pd.read_sql_query(\"SELECT * FROM temperature_data WHERE Method = 'HRT' AND (\"+\n                        \" OR \".join([\"Nr = {}\".format(n) for n in borehole_numbers])+\")\", conn)\nboreholes = pd.read_sql_query(\"select * from borehole_information_temperatures where \"+\n                              \" OR \".join([\" Nr = {}\".format(n) for n in borehole_numbers]), conn)\n\n\nfig = plt.figure(figsize=[10,8])\nplt.scatter(valids['Temperature'], valids['Depth_asl'], s=50, c=valids['Nr'], cmap='tab10', alpha=.5)\nplt.xlabel('Temperature [\u00b0C]')\nplt.ylabel('Depth [m asl]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This leaves all log measurements and sorts out BHT values, for instance. While `AND`, `OR` are the standard expressions for specifying different queries to be matched, \nthere are many more useful query statements. There are multiple resources to list available SQL commands and queries, e.g. \non `codeacademy <https://www.codecademy.com/learn/learn-sql/modules/learn-sql-queries/reference>`_ or on `bitdegree <https://www.bitdegree.org/learn/sql-commands-list>`_.  \nTo better distinguish the boreholes, let's add a legend to the plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = \"Paired\"\ncmap = get_cmap(name)  # type: matplotlib.colors.ListedColormap\ncolors = cmap.colors  # type: list\n\nfig, ax = plt.subplots(figsize=[16,12])\nax.set_prop_cycle(color=colors)\nfor i in borehole_numbers:\n    info = pd.read_sql_query(\"select * from borehole_information_temperatures where Nr = {}\".format(i), conn)\n    df = pd.read_sql_query(\"select * from temperature_data where Nr = {} and Method = 'HRT';\".format(i), conn)\n    ax.plot(df['Temperature'], -df['Depth']+info['Z'][0], '^', label=info['Namenach'][0], alpha=.6)\nax.set_ylabel('depth [m]')\nax.set_title('temperature [\u00b0C]')\nax.legend(loc='upper right',bbox_to_anchor=(1.32, 1.01),ncol=1)\nax.xaxis.tick_top()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A word on data distribution  \n\nUntil now, this notebook mainly dealt with the technical aspects of working with a database. It should provide the basic tools to perform actual analysis on the stored data. \nIn preparation for another notebook, we analyse the distribution of data, to answer for example the question: *How probable is a temperature of X \u00b0C at a certain depth of Y km, according to our data?*  \nThis may be done with another query, yielding all temperatures in a pre-defined depth bracket, for example. Another method for a quick estimate of data distribution is, to calculate \nthe `Kernel Density Estimate <https://mathisonian.github.io/kde/>`_) which, as the name says, is an estimate of a function underlying a certain distribution. Mathematically, it can be written as:  \n\n$$ f(x) = \\sum_i K \\bigg(\\frac{x-i}{bw}\\bigg) $$ \n\nWhere $K$ is the *Kernel* or *Kernel function*, and $bw$ the *bandwidth*. The higher the bandwith, the smoother the resulting KDE, as it controls the distance, at which data points contribute to the \ncurrent KDE-value. That is, a smaller bandwidth yields a more erratic KDE, while a high bandwidth value yields a smooth, yet shallower KDE where more distant points are taken into account.  \n\nHere, we use the `scipy <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html>`_ implementation of a gaussian KDE. This means, $K$ is a gaussian Kernel. \nThe bandwidth is estimated using a Scott estimate \\cite{scott1979}, which automatically estimates an appropriate bandwidth. \n\nIn the following lines, we set up a linear regression through all borehole data and visualize the data distribution by coloring the data by their KDE value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xreg = valids['Temperature'].values.reshape(-1,1)\nyreg = valids['Depth_asl'].values\nreg = LinearRegression().fit(xreg,yreg)\n\nr_sc = reg.score(xreg, yreg)\nprint('coefficient of determination:', r_sc)\n\nprint('intercept:', reg.intercept_)\n\nprint('slope:', reg.coef_)\n\n\nxy = np.vstack([valids['Temperature'], valids['Depth_asl']])\nz = gaussian_kde(xy)(xy)\nxreg = np.linspace(10,110,100)\nyreg = reg.coef_[0] * xreg + reg.intercept_\n\n# sphinx_gallery_thumbnail_number = 6\nfig, ax = plt.subplots(figsize=[16,10])\n\ncs = ax.scatter(valids['Temperature'], valids['Depth_asl'], c=z, s=70, alpha=.3, label='data')\nax.plot(xreg, yreg, 'k--', linewidth=3, label='regression')\nax.set_ylabel('depth [m a.s.l.]')\nax.set_title('temperature [\u00b0C]')\nax.xaxis.tick_top()\nax.text(88, 200, 'grad T = {:.3f} K/km'.format(np.abs(reg.coef_[0])), fontsize=18)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As to be expected from averaging temperature-depth data from multiple boreholes, the resulting temperature gradient reflects a normal continental temperature gradient. \nThis indicates, that there is no regional-scale source which would act as a heat-source and would thus regionally increase temperature gradients, and by that the (conductive) heat flow. \nFurther, the majority of data can be found between 600 m and 1000 m depth between 50 \u00b0C and 60 \u00b0C. \nIt should be noted, that the kind of borehole has to be considered, when looking at data distribution. \nIf borehole heat exchangers (BHE) are incorporated in the database, the most data will be at shallower depths, as BHEs usually extend to depths of around 200 m.  \n\n## One last information about databases\nIn this notebook, we worked with an SQL-database. This includes the standard steps of:  \n* connecting to a database \n* querying data from the database \n* analyzing data, adding / manipulating data, ...\n* closing the database\nThe last thing is important, as unexpected closure of non-closed databases may potentially corrupt them. So, the last step in working with the database is close it, as done in the following cell:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.close()\nconn.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}