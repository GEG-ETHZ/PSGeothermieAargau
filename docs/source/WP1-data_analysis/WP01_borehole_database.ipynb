{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Temperature database usage\r\n\r\nIn the following, I present ways to interact with a database of borehole\r\ntemperatures in the study area, using the module\r\n[db_access]{.title-ref}. These temperature data are used in other\r\nworkpackages, e.g. for model calibration or MC-rejection.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Borehole Database\r\n\r\nIt is no surprise that the core of a data-driven project is data. Data\r\nwhich is organized in a specific manner, so that users can easily\r\naccess, analyse, and manipulate data. Many different schemes exist, in\r\nwhich data can be organized, with a most common one being\r\nspreadsheet-like tables. Thus, spreadsheet software like Microsoft Excel\r\nor Libre-Office Calc are among popular solutions, when it comes to\r\nworking with data.\r\n\r\nWith growing amount of data, however, these software solutions may soon\r\nmeet their limits, as they can get overly complicated. One example would\r\nbe many `.xls` files, which are connected among each other using\r\nhyperlinks. This is obviously an error-prone solution, not really\r\npractical. Thus, greater amounts of data with a more complex structure,\r\nare usually maintained in a\r\n[database](https://en.wikipedia.org/wiki/Database), following a certain\r\n[data model](https://en.wikipedia.org/wiki/Data_model). Here, we use\r\n[SQLITE](https://www.sqlite.org/index.html) \\[1\\] as underlying database\r\nsolution, a SQL database engine.\r\n\r\nIn the following, we will briefly describe the database structure, its\r\ncontent and provide some short examples how to access the database and\r\nwork with the stored data.\r\n\r\n## Data model\r\n\r\nWithin the database, we follow, as we use SQL, a [relational\r\nmodel](https://en.wikipedia.org/wiki/Relational_model) to organize\r\nstored data. This data comprises mainly borehole temperature\r\nmeasurements in the study area. The data was originally compiled by\r\nSch\u00e4rli and Kohl \\[2\\] in a set of excel tables. This *original* data,\r\ni.e. in its excel form, is available as supplementary material to the\r\nNAGRA Working report [NAB\r\n12-61](https://www.nagra.ch/de/cat/publikationen/arbeitsberichte-nabs/nabs-2012/downloadcenter.htm).\r\nThis report comprises temperature measurements for boreholes all over\r\nSwitzerland. Additionally, a stratigraphical description is available\r\nfor some boreholes.\r\n\r\nMany of the temperature data from these deep boreholes is compiled in\r\nSch\u00e4rli and Kohl \\[2\\], in addition to temperature data from *shallow*\r\nboreholes, i.e. shallower than 500 m. In this work, we use a subset of\r\nthis data which is (**a**) inside our area of interest, and (**b**)\r\npublicly available data. For instance, the map further down shows a\r\nsubset of deep boreholes (triangles) in the study area, colored by the\r\ndata restriction. While blue represents open data, boreholes colored in\r\nred contain confidential data. Within the database, this information is\r\nstored, so confidential data can easily be erased from the database, in\r\ncase it is made public.\r\n\r\nCurrently, the database contains three related tables: \\* general\r\nborehole information (coordinates, name, original source, \\...) \\*\r\ntemperature depth information for all boreholes \\* available\r\npetrophysical information\r\n\r\nNot much petrophysical data is available from the boreholes. Temperature\r\ndepth information, however, comprises more than 39000 data points. In\r\nthe following, we present methods and procedures to access these data\r\nand work with it from within this notebook. For this, we use a mixture\r\nof SQL queries and methods of the data analysis library\r\n[pandas](https://pandas.pydata.org/).\r\n\r\n## Acessing data and visualizing\r\n\r\nQuerying a database is maybe the most often performed task, when it\r\ncomes to databases. When you type something in a seach bar, for example,\r\nyou query a database for the words you are looking for. The same, though\r\nin a more rudimentary form, can be done with the compiled \\\"borehole\r\ntemperature\\\" database.\r\n\r\nThe following code cells in this notebook show how: - to connect to the\r\ndatabase - introduces a very small library `db_access` - get information\r\nabout available tables in the database - formulate queries to get\r\ndesired data, e.g. temperature depth pairs for a specific borehole -\r\nstore query results in a pandas dataframe and visualize them\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nimport platform\nsys.path.append('../..')\nimport OpenWF.db_access as db_access\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "relative path to the .db file, which is the actual database\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "os_name = platform.system()\nif os_name=='Linux':\n    db_path = '../../../../ETHeatflow/dbase_model_btemps.db'\nelif os_name=='Windows':\n    db_path = '../../../../db_borehole_temps/dbase_model_btemps.db'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "connect to the database and get information about stored tables with the\r\n`connect` routine\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conn, c = db_access.connect(db_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we successfully connected to the database. One next step\r\nwould be to see, what different tables are stored in the database.\r\n`db_access` provides you with methods to do so. Of course, one can\r\ndirectly use an SQL query to do so. For user convenience, such queries\r\nare wrapped in some python methods of `db_access`. For instance, let\\'s\r\ncheck the names of tables in the database:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\nprint(c.fetchall())\n\ndb_access.get_tables(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essentially, these two commands do the same thing. In the `db_access`\r\nmethod, the `c.execute` and `c.fetchall` commands are bundled in one\r\nmethod, `.get_tables()`. The result are the three tables: -\r\nborehole_information_temperatures - temperature_data (with one backup\r\ntable, marked with extension \\_bak) - sample_information_petrophysics\r\n\r\nIn its current state, [db_access]{.title-ref} comprises very basic query\r\nmethods. More specific data-queries still need to be done via the\r\n[c.execute]{.title-ref} and [c.fetchall]{.title-ref} chain which is\r\nextremely versatile. For instance, consider out of the over 30000 data\r\nentries, we want to get all temperature measurements for Borehole Nr.\r\n111.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT * FROM {tn} WHERE {idf}=111;\".format(tn='temperature_data', idf='Nr'))\nprint(c.fetchall())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the name of this borehole, we can relate to the table\r\n*borehole_information_temperatures* and query the name for the borehole\r\nwith Nr. 111 in the exact same way:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT {param} FROM {tn} WHERE {idf}=111;\".format(param='Namenach',\n                                                             tn='borehole_information_temperatures', idf='Nr'))\nprint(c.fetchall())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To know which columns are available to choose from as `{param}` in the `execute` command, we can either list names fetched by an `execute` command:\n\n\nnam = c.execute(\"select * from borehole_information_temperatures\")\nnames = list(map(lambda x: x[0], nam.description))\nprint(names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\... or use a [db_access]{.title-ref} method which returns this list of\r\ntable headers:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "db_access.get_columns(c,table='borehole_information_temperatures')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now back to the query above, where we asked the database to provide all\r\ndata for borehole *Riehen-1*, i.e. borehole Nr. 111. The query returns a\r\nlist of table rows fitting the query command. While usable, it is\r\ndifficult to read, at least for humans. This is, where pandas comes into\r\nplay. As an extensive data analysis library,\r\n[pandas](https://pandas.pydata.org/) provides a lot of tools to deal\r\nwith a database and present them in\r\n[dataframes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html),\r\nwhich can be displayed in a way more organized way. Below, we submit a\r\nquery for the temperature data for borehole Nr. 111 and display it.\r\nquery database for Borehole Nr. 111 and store it in the dataframe df.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.read_sql_query(\"select * from temperature_data where Nr = 111;\", conn)\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next to readability, another advantage of querying via pandas, and\r\nstoring the result in a dataframe, is visualization. Pandas features\r\nsome plotting functions, which can quickly plot parameters in a\r\ndataframe. For example, let\\'s plot [Depth]{.title-ref} versus\r\n\\`Temperature\\`:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df.plot.scatter(x='Temperature', y='Depth_asl', s=50)\nplt.show()\n\n# alternative syntax in classical matplotlib\nfig, ax = plt.subplots()\nax.scatter(df['Temperature'], -df['Depth'], s=50)\nax.set_ylabel('Depth [m]')\nax.set_xlabel('Temperature [\u00b0C]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colleagues at [Georesources Switzerland\r\nGroup](https://georessourcen.ethz.ch/en/#georesources-switzerland)\r\nevaluated temperature data from deep boreholes following certain\r\ncriteria. According to their analysis, a subset of the deep boreholes\r\ncontain enough data for a reliable heat-flow estimation. Boreholes\r\npassing this quality assessment are marked with white **+** in the\r\nfollowing map.\r\n\r\n![valid boreholes in the study area](../_static/BaseMap_boreholes_database_valid_loic.png){.with-shadow\r\n.with-shadow width=\"800px\"}\r\n\r\nIt should be noted, that data in these boreholes are all temperature\r\nlogs with a high enough data density to reliably assess a temperature\r\ngradient. Until now, SQL queries consisted mainly of `select * ...`\r\nwhere the `*` represents `*all*`, i.e. selecting everything (similar to\r\nan `ls *` listing every content of a folder in bash). If now we want to\r\nknow, for instance, all different Borehole numbers, which are the\r\ndatabase ID for each borehole, we can use `select distinct ...`.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_borehole_numbers = pd.read_sql_query(\"select distinct Nr from temperature_data;\", conn)\nall_borehole_numbers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Out of this distinct list, only a few passed the QA step by the\r\nGS-Group. The ID-Numbers of these boreholes are compiled in the list\r\nbelow: Here is a list of boreholes which passed the QA step\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "borehole_numbers = [9, 10, 12, 21, 77, 78, 108, 111, 112, 113, 116, 122, 128, 139]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chaining queries\r\n\r\nWhen one searches for multiple keywords in, for instance, a google\r\nsearch, this narrows the number of potential results. Similarly this can\r\nbe done when querying the database. As an example, let\\'s query all data\r\nfrom the boreholes, which pass the QA-workflow by the GS-Group, and plot\r\ntemperatures versus depth: For instance, we want to get all the\r\nboreholes marked as \\_[valid]() in one dataframe:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "valids = pd.read_sql_query(\"select * from temperature_data where \"+\n                        \" OR \".join([\"Nr = {}\".format(n) for n in borehole_numbers]), conn)\nboreholes = pd.read_sql_query(\"select * from borehole_information_temperatures where \"+\n                              \" OR \".join([\" Nr = {}\".format(n) for n in borehole_numbers]), conn)\n\nfig = plt.figure(figsize=[10,8])\nplt.scatter(valids['Temperature'], valids['Depth_asl'], s=50, c=valids['Nr'], cmap='tab10', alpha=.5)\nplt.xlabel('Temperature [\u00b0C]')\nplt.ylabel('Depth [m asl]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this plot, where temperature measurements are colored by borehole\r\nnumber, we see that the temperature measurements from different\r\nboreholes overall follow a similar gradient. There are, however,\r\nsingular points next to the dense cluster of continuous temperature\r\nlogs. These are data points from different measuring procedures, such as\r\n**B** ottom **H** ole **T** emperatures (BHTs). If one would like to\r\ninclude *only* temperature logs in a database query, this can easily be\r\ndone by extending the above chained query command with an `AND` keyword,\r\nso that a query would read:\r\n\r\n``` {.SQL}\r\nSELECT * FROM temperature_data WHERE Method = 'HRT' AND (Nr = ? OR Nr = ? OR ...);\r\n```\r\n\r\nThis method essentially queries if a temperature measurement belongs to\r\na borehole with the number specified in our `borehole_numbers` list, and\r\nif the measurement method is HRT.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "valids = pd.read_sql_query(\"SELECT * FROM temperature_data WHERE Method = 'HRT' AND (\"+\n                        \" OR \".join([\"Nr = {}\".format(n) for n in borehole_numbers])+\")\", conn)\nboreholes = pd.read_sql_query(\"select * from borehole_information_temperatures where \"+\n                              \" OR \".join([\" Nr = {}\".format(n) for n in borehole_numbers]), conn)\n\n\nfig = plt.figure(figsize=[10,8])\nplt.scatter(valids['Temperature'], valids['Depth_asl'], s=50, c=valids['Nr'], cmap='tab10', alpha=.5)\nplt.xlabel('Temperature [\u00b0C]')\nplt.ylabel('Depth [m asl]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This leaves all log measurements and sorts out BHT values, for instance.\r\nWhile [AND]{.title-ref}, [OR]{.title-ref} are the standard expressions\r\nfor specifying different queries to be matched, there are many more\r\nuseful query statements. There are multiple resources to list available\r\nSQL commands and queries, e.g. on\r\n[codeacademy](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-queries/reference)\r\nor on [bitdegree](https://www.bitdegree.org/learn/sql-commands-list). To\r\nbetter distinguish the boreholes, let\\'s add a legend to the plot.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = \"Paired\"\ncmap = get_cmap(name)  # type: matplotlib.colors.ListedColormap\ncolors = cmap.colors  # type: list\n\nfig, ax = plt.subplots(figsize=[16,12])\nax.set_prop_cycle(color=colors)\nfor i in borehole_numbers:\n    info = pd.read_sql_query(\"select * from borehole_information_temperatures where Nr = {}\".format(i), conn)\n    df = pd.read_sql_query(\"select * from temperature_data where Nr = {} and Method = 'HRT';\".format(i), conn)\n    ax.plot(df['Temperature'], -df['Depth']+info['Z'][0], '^', label=info['Namenach'][0], alpha=.6)\nax.set_ylabel('depth [m]')\nax.set_title('temperature [\u00b0C]')\nax.legend(loc='upper right',bbox_to_anchor=(1.32, 1.01),ncol=1)\nax.xaxis.tick_top()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A word on data distribution\r\n\r\nUntil now, this notebook mainly dealt with the technical aspects of\r\nworking with a database. It should provide the basic tools to perform\r\nactual analysis on the stored data. In preparation for another notebook,\r\nwe analyse the distribution of data, to answer for example the question:\r\n*How probable is a temperature of X \u00b0C at a certain depth of Y km,\r\naccording to our data?* This may be done with another query, yielding\r\nall temperatures in a pre-defined depth bracket, for example. Another\r\nmethod for a quick estimate of data distribution is, to calculate the\r\n[Kernel Density Estimate](https://mathisonian.github.io/kde/)) which, as\r\nthe name says, is an estimate of a function underlying a certain\r\ndistribution. Mathematically, it can be written as:\r\n\r\n$$f(x) = \\sum_i K \\bigg(\\frac{x-i}{bw}\\bigg)$$\r\n\r\nWhere $K$ is the *Kernel* or *Kernel function*, and $bw$ the\r\n*bandwidth*. The higher the bandwith, the smoother the resulting KDE, as\r\nit controls the distance, at which data points contribute to the current\r\nKDE-value. That is, a smaller bandwidth yields a more erratic KDE, while\r\na high bandwidth value yields a smooth, yet shallower KDE where more\r\ndistant points are taken into account.\r\n\r\nHere, we use the\r\n[scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html)\r\nimplementation of a gaussian KDE. This means, $K$ is a gaussian Kernel.\r\nThe bandwidth is estimated using a Scott estimate \\[3\\], which\r\nautomatically estimates an appropriate bandwidth.\r\n\r\nIn the following lines, we set up a linear regression through all\r\nborehole data and visualize the data distribution by coloring the data\r\nby their KDE value:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xreg = valids['Temperature'].values.reshape(-1,1)\nyreg = valids['Depth_asl'].values\nreg = LinearRegression().fit(xreg,yreg)\n\nr_sc = reg.score(xreg, yreg)\nprint('coefficient of determination:', r_sc)\n\nprint('intercept:', reg.intercept_)\n\nprint('slope:', reg.coef_)\n\n\nxy = np.vstack([valids['Temperature'], valids['Depth_asl']])\nz = gaussian_kde(xy)(xy)\nxreg = np.linspace(10,110,100)\nyreg = reg.coef_[0] * xreg + reg.intercept_\n\n# sphinx_gallery_thumbnail_number = 6\nfig, ax = plt.subplots(figsize=[16,10])\n\ncs = ax.scatter(valids['Temperature'], valids['Depth_asl'], c=z, s=70, alpha=.3, label='data')\nax.plot(xreg, yreg, 'k--', linewidth=3, label='regression')\nax.set_ylabel('depth [m a.s.l.]')\nax.set_title('temperature [\u00b0C]')\nax.xaxis.tick_top()\nax.text(88, 200, 'grad T = {:.3f} K/km'.format(np.abs(reg.coef_[0])), fontsize=18)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As to be expected from averaging temperature-depth data from multiple\r\nboreholes, the resulting temperature gradient reflects a normal\r\ncontinental temperature gradient. This indicates, that there is no\r\nregional-scale source which would act as a heat-source and would thus\r\nregionally increase temperature gradients, and by that the (conductive)\r\nheat flow. Further, the majority of data can be found between 600 m and\r\n1000 m depth between 50 \u00b0C and 60 \u00b0C. It should be noted, that the kind\r\nof borehole has to be considered, when looking at data distribution. If\r\nborehole heat exchangers (BHE) are incorporated in the database, the\r\nmost data will be at shallower depths, as BHEs usually extend to depths\r\nof around 200 m.\r\n\r\n# One last information about databases\r\n\r\nIn this notebook, we worked with an SQL-database. This includes the\r\nstandard steps of: \\* connecting to a database \\* querying data from the\r\ndatabase \\* analyzing data, adding / manipulating data, \\... \\* closing\r\nthe database The last thing is important, as unexpected closure of\r\nnon-closed databases may potentially corrupt them. So, the last step in\r\nworking with the database is close it, as done in the following cell:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.close()\nconn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\r\n\r\n\\[1\\] Hipp, D. R., Kennedy, D., & Mistachkin, J. (2010). Sqlite\r\ndocumentation. \\[2\\] Sch\u00e4rli, U., & Kohl, T. (2002). Archivierung und\r\nKompilation geothermischer Daten der Schweiz und angrenzender Gebiete.\r\nSchweizerische Geophysikalische Kommission. \\[3\\] Scott, D. W. (1979).\r\nOn optimal and data-based histograms. Biometrika, 66(3), 605-610.\r\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}