{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the temperature database\n==============================\n\nIn the following, I present ways to interact with a database of borehole\ntemperatures in the study area, using the module\n[db_access]{.title-ref}. These temperature data are used in other\nworkpackages, e.g. for model calibration or MC-rejection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Borehole Database\n=================\n\nIt is no surprise that the core of a data-driven project is data. Data\nwhich is organized in a specific manner, so that users can easily\naccess, analyse, and manipulate data. Many different schemes exist, in\nwhich data can be organized, with a most common one being\nspreadsheet-like tables. Thus, spreadsheet software like Microsoft Excel\nor Libre-Office Calc are among popular solutions, when it comes to\nworking with data.\n\nWith growing amount of data, however, these software solutions may soon\nmeet their limits, as they can get overly complicated. One example would\nbe many [.xls]{.title-ref} files, which are connected among each other\nusing hyperlinks. This is obviously an error-prone solution, not really\npractical. Thus, greater amounts of data with a more complex structure,\nare usually maintained in a\n[database](https://en.wikipedia.org/wiki/Database), following a certain\n[data model](https://en.wikipedia.org/wiki/Data_model). Here, we use\n[SQLITE](https://www.sqlite.org/index.html) cite{hipp_sqlite_2019} as\nunderlying database solution, a SQL database engine. \\# In the\nfollowing, we will briefly describe the database structure, its content\nand provide some short examples how to access the database and work with\nthe stored data.\n\nData model\n----------\n\nWithin the database, we follow, as we use SQL, a [relational\nmodel](https://en.wikipedia.org/wiki/Relational_model) to organize\nstored data. This data comprises mainly borehole temperature\nmeasurements in the study area. The data was originally compiled by\nSch\u00e4rli and Kohl cite{scharli_archivierung_2002} in a set of excel\ntables. This *original* data, i.e. in its excel form, is available as\nsupplementary material to the NAGRA Working report [NAB\n12-61](https://www.nagra.ch/de/cat/publikationen/arbeitsberichte-nabs/nabs-2012/downloadcenter.htm).\nThis report comprises temperature measurements for boreholes all over\nSwitzerland. Additionally, a stratigraphical description is available\nfor some boreholes. Figure ref{fig:borehole_map} shows boreholes in\nSwitzerland, which are deeper than 500 m.\n\nMany of the temperature data from these deep boreholes is compiled in\nSch\u00e4rli and Kohl cite{scharli_archivierung_2002}, in addition to\ntemperature data from *shallow* boreholes, i.e. shallower than 500 m. In\nthis work, we use a subset of this data which is (**a**) inside our area\nof interest, and (**b**) publicly available data. For instance, figure\nref{fig:database_map} shows a subset of deep boreholes (triangles) in\nthe study area, colored by the data restriction. While blue represents\nopen data, boreholes colored in red contain confidential data. Within\nthe database, this information is stored, so confidential data can\neasily be erased from the database, in case it is made public.\n\nCurrently, the database contains three related tables: \\* general\nborehole information (coordinates, name, original source, \\...) \\*\ntemperature depth information for all boreholes \\* available\npetrophysical information\n\nNot much petrophysical data is available from the boreholes. Temperature\ndepth information, however, comprises more than 39000 data points. In\nthe following, we present methods and procedures to access these data\nand work with it from within this notebook. For this, we use a mixture\nof SQL queries and methods of the data analysis library\n[pandas](https://pandas.pydata.org/).\n\nAcessing data and visualizing\n-----------------------------\n\nQuerying a database is maybe the most often performed task, when it\ncomes to databases. When you type something in a seach bar, for example,\nyou query a database for the words you are looking for. The same, though\nin a more rudimentary form, can be done with the compiled \\\"borehole\ntemperature\\\" database.\n\nThe following code cells in this notebook show how: \\* to connect to the\ndatabase \\* introduces a very small library [db_access]{.title-ref} \\*\nget information about available tables in the database \\* formulate\nqueries to get desired data, e.g. temperature depth pairs for a specific\nborehole \\* store query results in a pandas dataframe and visualize them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nsys.path.append('../..')\nimport OpenWF.db_access as db_access\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\nfrom scipy.stats import gaussian_kde\nfrom sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "relative path to the .db file, which is the actual database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "db_path = '../../../../ETHeatflow/dbase_model_btemps.db'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "connect to the database and get information about stored tables with the\n`connect` routine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conn, c = db_access.connect(db_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we successfully connected to the database. One next step\nwould be to see, what different tables are stored in the database.\n[db_access]{.title-ref} provides you with methods to do so. Of course,\none can directly use an SQL query to do so. For user convenience, such\nqueries are wrapped in some python methods of [db_access]{.title-ref}.\nFor instance, let\\'s check the names of tables in the database:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\nprint(c.fetchall())\n\ndb_access.get_tables(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essentially, these two commands do the same thing. In the\n[db_access]{.title-ref} method, the [c.execute]{.title-ref} and\n[c.fetchall]{.title-ref} commands are bundled in one method,\n[.get_tables()]{.title-ref}. The result are the three tables: \\*\nborehole_information_temperatures \\* temperature_data (with one backup\ntable, marked with extension \\_bak) \\* sample_information_petrophysics\n\nIn its current state, [db_access]{.title-ref} comprises very basic query\nmethods. More specific data-queries still need to be done via the\n[c.execute]{.title-ref} and [c.fetchall]{.title-ref} chain which is\nextremely versatile. For instance, consider out of the over 30000 data\nentries, we want to get all temperature measurements for Borehole Nr.\n111.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT * FROM {tn} WHERE {idf}=111;\".format(tn='temperature_data', idf='Nr'))\nprint(c.fetchall())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the name of this borehole, we can relate to the table\n*borehole_information_temperatures* and query the name for the borehole\nwith Nr. 111 in the exact same way:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.execute(\"SELECT {param} FROM {tn} WHERE {idf}=111;\".format(param='Namenach',\n                                                             tn='borehole_information_temperatures', idf='Nr'))\nprint(c.fetchall())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To know which columns are available to choose from as `{param}` in the `execute` command, we can either list names fetched by an `execute` command:\n\n\nnam = c.execute(\"select * from borehole_information_temperatures\")\nnames = list(map(lambda x: x[0], nam.description))\nprint(names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\... or use a [db_access]{.title-ref} method which returns this list of\ntable headers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "db_access.get_columns(c,table='borehole_information_temperatures')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now back to the query above, where we asked the database to provide all\ndata for borehole *Riehen-1*, i.e. borehole Nr. 111. The query returns a\nlist of table rows fitting the query command. While usable, it is\ndifficult to read, at least for humans. This is, where pandas comes into\nplay. As an extensive data analysis library,\n[pandas](https://pandas.pydata.org/) provides a lot of tools to deal\nwith a database and present them in\n[dataframes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html),\nwhich can be displayed in a way more organized way. Below, we submit a\nquery for the temperature data for borehole Nr. 111 and display it.\nquery database for Borehole Nr. 111 and store it in the dataframe df.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.read_sql_query(\"select * from temperature_data where Nr = 111;\", conn)\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next to readability, another advantage of querying via pandas, and\nstoring the result in a dataframe, is visualization. Pandas features\nsome plotting functions, which can quickly plot parameters in a\ndataframe. For example, let\\'s plot [Depth]{.title-ref} versus\n\\`Temperature\\`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df.plot.scatter(x='Temperature', y='Depth_asl', s=50)\nplt.show()\n\n# alternative syntax in classical matplotlib\nfig, ax = plt.subplots()\nax.scatter(df['Temperature'], -df['Depth'], s=50)\nax.set_ylabel('Depth [m]')\nax.set_xlabel('Temperature [\u00b0C]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colleagues at [Georesources Switzerland\nGroup](https://georessourcen.ethz.ch/en/#georesources-switzerland)\nevaluated temperature data from deep boreholes following certain\ncriteria. According to their analysis, a subset of the deep boreholes\ncontain enough data for a reliable heat-flow estimation. Boreholes\npassing this quality assessment are marked with white **+** in Figure\nref{fig:chosen_boreholes}.\n\n\\<hr\\> begin{figure}\nincludegraphics\\[width=10cm\\]{<https://i.ibb.co/m5P5fCc/Base-Map-boreholes-database-valid-loic.png>}\ncaption{label{fig:chosen_boreholes} Map of the study area, similar to\nFigure 2. Boreholes passing the quality assessment step are marked with\nwhite **+**.} end{figure} \\<hr\\>\n\nIt should be noted, that data in these boreholes are all temperature\nlogs with a high enough data density to reliably assess a temperature\ngradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Until now, SQL queries consisted mainly of `select * ...` where the * represents *all*, i.e. selecting everything (similar to an `ls *` listing every content of a folder in bash). \n# If now we want to know, for instance, all different Borehole numbers, which are the database ID for each borehole, we can use `select distinct ...`.\n\nall_borehole_numbers = pd.read_sql_query(\"select distinct Nr from temperature_data;\", conn)\nall_borehole_numbers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Out of this distinct list, only a few passed the QA step by the\nGS-Group. The ID-Numbers of these boreholes are compiled in the list\nbelow: Here is a list of boreholes which passed the QA step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "borehole_numbers = [9, 10, 12, 21, 77, 78, 108, 111, 112, 113, 116, 122, 128, 139]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chaining queries\n================\n\nWhen one searches for multiple keywords in, for instance, a google\nsearch, this narrows the number of potential results. Similarly this can\nbe done when querying the database. As an example, let\\'s query all data\nfrom the boreholes, which pass the QA-workflow by the GS-Group, and plot\ntemperatures versus depth: For instance, we want to get all the\nboreholes marked as \\_[valid]() in one dataframe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "valids = pd.read_sql_query(\"select * from temperature_data where \"+\n                        \" OR \".join([\"Nr = {}\".format(n) for n in borehole_numbers]), conn)\nboreholes = pd.read_sql_query(\"select * from borehole_information_temperatures where \"+\n                              \" OR \".join([\" Nr = {}\".format(n) for n in borehole_numbers]), conn)\n\nfig = plt.figure(figsize=[10,8])\nplt.scatter(valids['Temperature'], valids['Depth_asl'], s=50, c=valids['Nr'], cmap='tab10', alpha=.5)\nplt.xlabel('Temperature [\u00b0C]')\nplt.ylabel('Depth [m asl]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this plot, where temperature measurements are colored by borehole\nnumber, we see that the temperature measurements from different\nboreholes overall follow a similar gradient. There are, however,\nsingular points next to the dense cluster of continuous temperature\nlogs. These are data points from different measuring procedures, such as\n**B**ottom **H**ole **T**emperatures (BHTs). If one would like to\ninclude *only* temperature logs in a database query, this can easily be\ndone by extending the above chained query command with an\n[AND]{.title-ref} keyword, so that a query would read:\n`` `SQL SELECT * FROM temperature_data WHERE Method = 'HRT' AND (Nr = ? OR Nr = ? OR ...); ``[\nThis method essentially queries if a temperature measurement belongs to\na borehole with the number specified in our\n\\`borehole_numbers]{.title-ref} list, and if the measurement method is\nHRT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "valids = pd.read_sql_query(\"SELECT * FROM temperature_data WHERE Method = 'HRT' AND (\"+\n                        \" OR \".join([\"Nr = {}\".format(n) for n in borehole_numbers])+\")\", conn)\nboreholes = pd.read_sql_query(\"select * from borehole_information_temperatures where \"+\n                              \" OR \".join([\" Nr = {}\".format(n) for n in borehole_numbers]), conn)\n\n\nfig = plt.figure(figsize=[10,8])\nplt.scatter(valids['Temperature'], valids['Depth_asl'], s=50, c=valids['Nr'], cmap='tab10', alpha=.5)\nplt.xlabel('Temperature [\u00b0C]')\nplt.ylabel('Depth [m asl]')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This leaves all log measurements and sorts out BHT values, for instance.\nWhile [AND]{.title-ref}, [OR]{.title-ref} are the standard expressions\nfor specifying different queries to be matched, there are many more\nuseful query statements. There are multiple resources to list available\nSQL commands and queries, e.g. on\n[codeacademy](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-queries/reference)\nor on [bitdegree](https://www.bitdegree.org/learn/sql-commands-list). To\nbetter distinguish the boreholes, let\\'s add a legend to the plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = \"Paired\"\ncmap = get_cmap(name)  # type: matplotlib.colors.ListedColormap\ncolors = cmap.colors  # type: list\n\nfig, ax = plt.subplots(figsize=[16,12])\nax.set_prop_cycle(color=colors)\nfor i in borehole_numbers:\n    info = pd.read_sql_query(\"select * from borehole_information_temperatures where Nr = {}\".format(i), conn)\n    df = pd.read_sql_query(\"select * from temperature_data where Nr = {} and Method = 'HRT';\".format(i), conn)\n    ax.plot(df['Temperature'], -df['Depth']+info['Z'][0], '^', label=info['Namenach'][0], alpha=.6)\nax.set_ylabel('depth [m]')\nax.set_title('temperature [\u00b0C]')\nax.legend(loc='upper right',bbox_to_anchor=(1.32, 1.01),ncol=1)\nax.xaxis.tick_top()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A word on data distribution\n===========================\n\nUntil now, this notebook mainly dealt with the technical aspects of\nworking with a database. It should provide the basic tools to perform\nactual analysis on the stored data. In preparation for another notebook,\nwe analyse the distribution of data, to answer for example the question:\n*How probable is a temperature of X \u00b0C at a certain depth of Y km,\naccording to our data?* This may be done with another query, yielding\nall temperatures in a pre-defined depth bracket, for example. Another\nmethod for a quick estimate of data distribution is, to calculate the\n[Kernel Density Estimate](https://mathisonian.github.io/kde/)) which, as\nthe name says, is an estimate of a function underlying a certain\ndistribution. Mathematically, it can be written as:\n\n\\$\\$ f(x) = sum_i K bigg(frac{x-i}{bw}bigg) \\$\\$\n\nWhere \\$K\\$ is the *Kernel* or *Kernel function*, and \\$bw\\$ the\n*bandwidth*. The higher the bandwith, the smoother the resulting KDE, as\nit controls the distance, at which data points contribute to the current\nKDE-value. That is, a smaller bandwidth yields a more erratic KDE, while\na high bandwidth value yields a smooth, yet shallower KDE where more\ndistant points are taken into account.\n\nHere, we use the\n[scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html)\nimplementation of a gaussian KDE. This means, \\$K\\$ is a gaussian\nKernel. The bandwidth is estimated using a Scott estimate\ncite{scott1979}, which automatically estimates an appropriate bandwidth.\n\nIn the following lines, we set up a linear regression through all\nborehole data and visualize the data distribution by coloring the data\nby their KDE value:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xreg = valids['Temperature'].values.reshape(-1,1)\nyreg = valids['Depth_asl'].values\nreg = LinearRegression().fit(xreg,yreg)\n\nr_sc = reg.score(xreg, yreg)\nprint('coefficient of determination:', r_sc)\n\nprint('intercept:', reg.intercept_)\n\nprint('slope:', reg.coef_)\n\n\nxy = np.vstack([valids['Temperature'], valids['Depth_asl']])\nz = gaussian_kde(xy)(xy)\nxreg = np.linspace(10,110,100)\nyreg = reg.coef_[0] * xreg + reg.intercept_\n\n# sphinx_gallery_thumbnail_number = 6\nfig, ax = plt.subplots(figsize=[16,10])\n\ncs = ax.scatter(valids['Temperature'], valids['Depth_asl'], c=z, s=70, alpha=.3, label='data')\nax.plot(xreg, yreg, 'k--', linewidth=3, label='regression')\nax.set_ylabel('depth [m a.s.l.]')\nax.set_title('temperature [\u00b0C]')\nax.xaxis.tick_top()\nax.text(88, 200, 'grad T = {:.3f} K/km'.format(np.abs(reg.coef_[0])), fontsize=18)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As to be expected from averaging temperature-depth data from multiple\nboreholes, the resulting temperature gradient reflects a normal\ncontinental temperature gradient. This indicates, that there is no\nregional-scale source which would act as a heat-source and would thus\nregionally increase temperature gradients, and by that the (conductive)\nheat flow. Further, the majority of data can be found between 600 m and\n1000 m depth between 50 \u00b0C and 60 \u00b0C. It should be noted, that the kind\nof borehole has to be considered, when looking at data distribution. If\nborehole heat exchangers (BHE) are incorporated in the database, the\nmost data will be at shallower depths, as BHEs usually extend to depths\nof around 200 m.\n\nOne last information about databases\n====================================\n\nIn this notebook, we worked with an SQL-database. This includes the\nstandard steps of: \\* connecting to a database \\* querying data from the\ndatabase \\* analyzing data, adding / manipulating data, \\... \\* closing\nthe database The last thing is important, as unexpected closure of\nnon-closed databases may potentially corrupt them. So, the last step in\nworking with the database is close it, as done in the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c.close()\nconn.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}